{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km9h352_WIdj"
      },
      "source": [
        "# Datathon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg2SPxZY6gND"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL9GTnv76fnr"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_ROWS         =     1000   # Number of rows to keep (faster execution)\n",
        "MIN_SIMILARITY_COEF    =     0.99   # Minimal coefficent of similarity to keep similar words (between 0 and 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmDRGZTwWMcz"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_gTSKztDPsi",
        "outputId": "64a482ff-356d-4d9a-cb4e-8b4271f24588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.2)\n",
            "Collecting textblob_fr\n",
            "  Downloading textblob_fr-0.2.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.2/561.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: textblob>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from textblob_fr) (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob>=0.8.0->textblob_fr) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.8.0->textblob_fr) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.8.0->textblob_fr) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.8.0->textblob_fr) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.8.0->textblob_fr) (4.66.1)\n",
            "Installing collected packages: textblob_fr\n",
            "Successfully installed textblob_fr-0.2.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install plotly\n",
        "! pip install textblob_fr\n",
        "! pip install spacy\n",
        "! pip install graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0meEifYTV-r4"
      },
      "source": [
        "## Clone le répo git\n",
        "\n",
        "[Repo Git setup SPARK](https://gist.github.com/javieraespinosa/0e30bccba30caf24042b7b189e4b4c36)\n",
        "\n",
        "**NOTE:** Using Java 8 instead of 11 to be able too use sparknlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wedSZ5CMf9Xl",
        "outputId": "d1bc117a-6c2d-4966-d960-1b7e4816d0a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to cloud\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to cloud\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [632 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [47.6 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,265 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,292 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,494 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,535 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,520 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
            "Fetched 8,159 kB in 2s (4,196 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk-headless is already the newest version (11.0.21+9-0ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n",
            "Collecting pyspark==3.0.0\n",
            "  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.7/204.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Collecting py4j==0.10.9 (from pyspark==3.0.0)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044160 sha256=41b921ecac20f47f38999b165d7db36a09b2c4b843a1127b2d59a711d60519f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/bb/8b/ca24d3f756f2ed967225b0871898869db676eb5846df5adc56\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, findspark, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed findspark-2.0.1 py4j-0.10.9 pyspark-3.0.0\n",
            "--2023-11-30 21:15:07--  https://github.com/archivesunleashed/aut/releases/download/aut-0.91.0/aut-0.91.0.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/96417459/99fe4f91-6039-4225-848a-ee82c86b9200?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231130T211507Z&X-Amz-Expires=300&X-Amz-Signature=a6b3fe8ea251768de4fab6a881b311b275a49b0aa36e6a76c752175ac7486b6d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=96417459&response-content-disposition=attachment%3B%20filename%3Daut-0.91.0.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-30 21:15:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/96417459/99fe4f91-6039-4225-848a-ee82c86b9200?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231130T211507Z&X-Amz-Expires=300&X-Amz-Signature=a6b3fe8ea251768de4fab6a881b311b275a49b0aa36e6a76c752175ac7486b6d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=96417459&response-content-disposition=attachment%3B%20filename%3Daut-0.91.0.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2841 (2.8K) [application/octet-stream]\n",
            "Saving to: ‘aut-0.91.0.zip’\n",
            "\n",
            "aut-0.91.0.zip      100%[===================>]   2.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-30 21:15:07 (38.2 MB/s) - ‘aut-0.91.0.zip’ saved [2841/2841]\n",
            "\n",
            "--2023-11-30 21:15:07--  https://github.com/archivesunleashed/aut/releases/download/aut-0.91.0/aut-0.91.0-fatjar.jar\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/96417459/d3270f57-4b11-4c48-9c77-b6f6a4823c0d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231130T211507Z&X-Amz-Expires=300&X-Amz-Signature=ec422d075f0b9eadee98f22b5f52923ae701353eb1a780b6de3b45910f0d2cf1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=96417459&response-content-disposition=attachment%3B%20filename%3Daut-0.91.0-fatjar.jar&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-30 21:15:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/96417459/d3270f57-4b11-4c48-9c77-b6f6a4823c0d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231130T211507Z&X-Amz-Expires=300&X-Amz-Signature=ec422d075f0b9eadee98f22b5f52923ae701353eb1a780b6de3b45910f0d2cf1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=96417459&response-content-disposition=attachment%3B%20filename%3Daut-0.91.0-fatjar.jar&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 183075070 (175M) [application/octet-stream]\n",
            "Saving to: ‘aut-0.91.0-fatjar.jar’\n",
            "\n",
            "aut-0.91.0-fatjar.j 100%[===================>] 174.59M   191MB/s    in 0.9s    \n",
            "\n",
            "2023-11-30 21:15:08 (191 MB/s) - ‘aut-0.91.0-fatjar.jar’ saved [183075070/183075070]\n",
            "\n",
            "--2023-11-30 21:15:09--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.0-s_2.12/graphframes-0.8.2-spark3.0-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 18.172.170.17, 18.172.170.21, 18.172.170.23, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|18.172.170.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247882 (242K) [binary/octet-stream]\n",
            "Saving to: ‘graphframes-0.8.2-spark3.0-s_2.12.jar’\n",
            "\n",
            "graphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-11-30 21:15:09 (8.42 MB/s) - ‘graphframes-0.8.2-spark3.0-s_2.12.jar’ saved [247882/247882]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "#------------------------------------------\n",
        "# Versions\n",
        "#------------------------------------------\n",
        "SPARK_VERSION  = \"3.0.0\"\n",
        "JAVA_VERSION   = \"11\"\n",
        "AUT_VERSION    = \"0.91.0\"\n",
        "\n",
        "GRAPHFRAME_VERSION = \"0.8.2\"\n",
        "GRAPHFRAME_SCALA_VERSION = \"2.12\"\n",
        "\n",
        "#------------------------------------------\n",
        "# Folders\n",
        "#------------------------------------------\n",
        "APPS_HOME = \"apps\"\n",
        "APPS_HOME = os.path.join(os.getcwd(), APPS_HOME)\n",
        "!mkdir -p \"$APPS_HOME\"\n",
        "!rm -rf sample_data   #remove colab default folder\n",
        "\n",
        "\n",
        "#------------------------------------------\n",
        "# JAVA JDK\n",
        "#------------------------------------------\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y openjdk-\"$JAVA_VERSION\"-jdk-headless\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-{}-openjdk-amd64\".format(JAVA_VERSION)\n",
        "\n",
        "\n",
        "#------------------------------------------\n",
        "# SPARK\n",
        "#------------------------------------------\n",
        "!pip install \"pyspark==$SPARK_VERSION\" findspark\n",
        "SPARK_HOME = !python -c \"import pyspark as _; print(_.__path__)\"\n",
        "SPARK_HOME = SPARK_HOME[0][2:-2]\n",
        "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
        "\n",
        "\n",
        "#------------------------------------------\n",
        "# ARCHIVES UNLEASHED TOOLKIT\n",
        "#------------------------------------------\n",
        "!wget https://github.com/archivesunleashed/aut/releases/download/aut-\"$AUT_VERSION\"/aut-\"$AUT_VERSION\".zip\n",
        "!wget https://github.com/archivesunleashed/aut/releases/download/aut-\"$AUT_VERSION\"/aut-\"$AUT_VERSION\"-fatjar.jar\n",
        "!mv aut-* \"$APPS_HOME\"\n",
        "\n",
        "\n",
        "#------------------------------------------\n",
        "# GRAPHFRAME lib\n",
        "#------------------------------------------\n",
        "GRAPHFRAME_SPARK_VERSION = \"{}-spark{}-s_{}\".format(GRAPHFRAME_VERSION, SPARK_VERSION[:-2], GRAPHFRAME_SCALA_VERSION)\n",
        "\n",
        "!wget https://repos.spark-packages.org/graphframes/graphframes/\"$GRAPHFRAME_SPARK_VERSION\"/graphframes-\"$GRAPHFRAME_SPARK_VERSION\".jar\n",
        "!jar -xf   graphframes-\"$GRAPHFRAME_SPARK_VERSION\".jar graphframes\n",
        "!zip -q -r graphframes-\"$GRAPHFRAME_SPARK_VERSION\".zip graphframes\n",
        "!rm -r graphframes\n",
        "!mv graphframes-* \"$APPS_HOME\"\n",
        "\n",
        "\n",
        "#------------------------------------------\n",
        "# SPARK init\n",
        "#------------------------------------------\n",
        "import findspark\n",
        "\n",
        "SPARK_DRIVER_MEMORY   = \"8g\"\n",
        "\n",
        "JARS     = !find \"$APPS_HOME\" -maxdepth 1 -iname \"*.jar\"\n",
        "PY_FILES = !find \"$APPS_HOME\" -maxdepth 1 -iname \"*.zip\"\n",
        "\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--driver-memory {} --jars {} --py-files {} pyspark-shell\".format(\n",
        "    SPARK_DRIVER_MEMORY,\n",
        "    \",\".join(JARS),\n",
        "    \",\".join(PY_FILES)\n",
        ")\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "\n",
        "#------------------------------------------\n",
        "# SPARK session\n",
        "#------------------------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\")\\\n",
        "                                  .getOrCreate()\n",
        "\n",
        "# Backward compability with AUT toolkit\n",
        "sqlContext = SQLContext(spark.sparkContext, sparkSession=spark)\n",
        "sc = spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9rEKDOHDa1p"
      },
      "source": [
        "### Installing extenal libs\n",
        "\n",
        "\n",
        "\n",
        "*   plotly pour l'affichage\n",
        "*   textblob_fr pour l'analyse de sentiment de mots français\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3vTpNaUDwrM"
      },
      "source": [
        "### Importer les librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUftC2YgDFmL",
        "outputId": "fa2f54c2-489f-4010-e4db-6c0c1794e2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import words\n",
        "\n",
        "from tkinter.constants import SEPARATOR\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from textblob_fr import PatternTagger, PatternAnalyzer\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ3nuIgFQ95h",
        "outputId": "d12ce5f3-8361-4864-e09d-5dd110c79d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-30 21:15:22--  https://openlexicon.fr/datasets-info/Liste-de-mots-francais-Gutenberg/liste.de.mots.francais.frgut.txt\n",
            "Resolving openlexicon.fr (openlexicon.fr)... 185.199.108.153\n",
            "Connecting to openlexicon.fr (openlexicon.fr)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4058504 (3.9M) [text/plain]\n",
            "Saving to: ‘liste.de.mots.francais.frgut.txt’\n",
            "\n",
            "liste.de.mots.franc 100%[===================>]   3.87M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-11-30 21:15:23 (54.2 MB/s) - ‘liste.de.mots.francais.frgut.txt’ saved [4058504/4058504]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! mkdir dictionnary\n",
        "! wget https://openlexicon.fr/datasets-info/Liste-de-mots-francais-Gutenberg/liste.de.mots.francais.frgut.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1wqKg7xi43d"
      },
      "outputs": [],
      "source": [
        "with open(\"liste.de.mots.francais.frgut.txt\", 'r') as f:\n",
        "    french_words = [line.strip() for line in f]\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W0WgvUY9QaM"
      },
      "source": [
        "## Initialisation de SPARK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "84htTBLsGqEK",
        "outputId": "92662416-8d78-44b4-b401-a8acf6067a91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a022c186d70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://33ceb3fbb889:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OC9iNVwW4G5",
        "outputId": "87b6d0f8-bb25-4f3b-bef1-16315870131d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-30 21:15:31.953122: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-30 21:15:31.953204: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-30 21:15:31.953257: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-30 21:15:31.982119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-30 21:15:34.627279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting fr-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.6.0/fr_core_news_sm-3.6.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "! python3 -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJJYkxwve40U"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StopWordsRemover, Tokenizer, RegexTokenizer,IDF,Word2Vec,CountVectorizer\n",
        "from pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
        "from pyspark.ml import Pipeline, Transformer\n",
        "from pyspark.sql.functions import desc, col, udf, concat_ws, rand\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "from pyspark.ml.clustering import LDA\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "import numpy as np\n",
        "import graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S6O59WxEAAd"
      },
      "source": [
        "### Fonctions de traitement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WBiztQ5D_Jh"
      },
      "outputs": [],
      "source": [
        "def wordInDict(text) -> str:\n",
        "  retStr = \"\"\n",
        "\n",
        "  tempList = text.split()\n",
        "\n",
        "  for mot in tempList:\n",
        "    if(mot in french_words):\n",
        "      retStr += mot+\" \"\n",
        "\n",
        "  return retStr\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
        "    return blob.sentiment[0]\n",
        "\n",
        "def remove_duplicates(text):\n",
        "    words = text.split()\n",
        "    words = list(set(words))\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "\n",
        "def categoriseText(text) -> list:\n",
        "  doc = nlp(remove_duplicates(text))\n",
        "  return [str(entity.text +\" - \"+ entity.label_) for entity in doc.ents]\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
        "\n",
        "# Define a UDF to find the most frequent word\n",
        "def most_frequent(wordList):\n",
        "  if(wordList):\n",
        "    return Counter(wordList).most_common(1)[0][0]\n",
        "  else:\n",
        "    return \"null\"\n",
        "\n",
        "# Define a UDF to find the most frequent word\n",
        "def convertToColor(inputString):\n",
        "  tempFloat = float(inputString)\n",
        "  tempFloat += 0.5\n",
        "  contrTempFloat = 1 - tempFloat\n",
        "  text = \"{color1:.2f} {color2:.2f} 1.000\"\n",
        "  return text.format(color1=contrTempFloat,color2=tempFloat)\n",
        "\n",
        "def getURLDomain(url):\n",
        "    urlList = url.split('/')\n",
        "    return urlList[2]\n",
        "\n",
        "def getListLen(inputList):\n",
        "  return len(inputList)\n",
        "\n",
        "count_words_udf = udf(getListLen, IntegerType())\n",
        "\n",
        "convert_to_color = udf(convertToColor, StringType())\n",
        "get_url_domain_name = udf(getURLDomain, StringType())\n",
        "most_frequent_udf = udf(most_frequent, StringType())\n",
        "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
        "sentiment_udf = udf(analyze_sentiment, StringType())\n",
        "word_in_dict_udf = udf(wordInDict, StringType())\n",
        "cat_url_udf = udf(wordInDict, ArrayType(StringType()))\n",
        "categories_text = udf(categoriseText,ArrayType(StringType()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Category functions"
      ],
      "metadata": {
        "id": "gfBFIEst9gGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Champ lexical : Musique\n",
        "lexique_musique = ['mélodie', 'rythme', 'partition', 'harmonie', 'accord', 'instrument', 'symphonie', 'sonate', 'crescendo', 'soliste', 'orchestre', 'tempo', 'harmonica', 'improvisation', 'solfège', 'compositrice', 'cadence', 'concerto']\n",
        "\n",
        "# Champ lexical : Sport\n",
        "lexique_sport = ['athlète', 'compétition', 'score', 'entraînement', 'équipe', 'victoire', 'défaite', 'entraîneur', 'athlétisme', 'stratégie', 'arbitre', 'record', 'médaille', 'endurance', 'fair-play', 'sélection', 'dopage', 'marathon']\n",
        "\n",
        "# Champ lexical : Politique\n",
        "lexique_politique = ['politicien', 'élection', 'gouvernement', 'démocratie', 'législation', 'diplomatie', 'citoyenneté', 'débat', 'parti politique', 'constitution', 'lobbying', 'réforme', 'manifestation', 'coalition', 'discours', 'droits de l\\'homme']\n",
        "\n",
        "# Champ lexical : Gastronomie\n",
        "lexique_gastronomie = ['cuisine', 'recette', 'ingrédient', 'saveur', 'gourmet', 'chef', 'dégustation', 'épices', 'plat', 'gastronome', 'cuisinier', 'menu', 'assaisonnement', 'gourmandise', 'appétit']\n",
        "\n",
        "# Champ lexical : Voyage\n",
        "lexique_voyage = ['destination', 'aventure', 'exploration', 'itinéraire', 'voyageur', 'excursion', 'découverte', 'routard', 'paysage', 'culture', 'tourisme', 'séjour', 'évasion', 'hébergement', 'guide', 'aventureux']\n",
        "# Lexique Société\n",
        "lexique_societe = ['société', 'communauté', 'citoyen', 'norme', 'coutume', 'valeur', 'tradition', 'éthique', 'diversité', 'inclusion', 'minorité', 'majorité', 'identité', 'citoyenneté', 'engagement', 'solidarité', 'harmonie', 'justice', 'responsabilité', 'participation']\n",
        "\n",
        "# Lexique Nature\n",
        "lexique_nature = ['nature', 'environnement', 'écologie', 'biodiversité', 'écosystème', 'climat', 'faune', 'flore', 'forêt', 'océan', 'montagne', 'développement durable', 'conservation', 'énergie renouvelable', 'pollution', 'réchauffement climatique', 'sustainability', 'recyclage', 'agriculture durable']\n",
        "\n",
        "# Lexique Technologie\n",
        "lexique_technologie = ['technologie', 'innovation', 'numérique', 'intelligence artificielle', 'robotique', 'internet', 'programmation', 'algorithmes', 'cybersécurité', 'réseaux sociaux', 'télécommunications', 'géolocalisation', 'biotechnologie', 'nanotechnologie', 'realité virtuelle', 'cloud computing', 'smartphones', 'automatisation', 'drones', 'impression 3D']\n",
        "\n",
        "# Lexique Culture\n",
        "lexique_culture = ['culture', 'tradition', 'coutume', 'art', 'musée', 'littérature', 'cinéma', 'théâtre', 'danse', 'musique', 'cuisine', 'religion', 'festivals', 'cérémonie', 'langue', 'folklore', 'patrimoine', 'identité culturelle', 'diversité culturelle']\n",
        "\n",
        "# Lexique Économie\n",
        "lexique_economie = ['économie', 'commerce', 'entreprise', 'marché', 'produit intérieur brut (PIB)', 'investissement', 'commerce international', 'consommation', 'finance', 'banque', 'monnaie', 'pauvreté', 'chômage', 'croissance économique', 'développement économique', 'inflation', 'dette', 'budget', 'fiscalité']\n",
        "\n",
        "# Lexique Science\n",
        "lexique_science = ['science', 'recherche', 'scientifique', 'expérience', 'hypothèse', 'théorie', 'méthode scientifique', 'biologie', 'physique', 'chimie', 'astronomie', 'géologie', 'mathématiques', 'informatique', 'médecine', 'psychologie', 'écologie', 'génétique', 'évolution']\n",
        "\n",
        "# Lexique Relations\n",
        "lexique_relations = ['relations', 'communication', 'interpersonnel', 'confiance', 'respect', 'coopération', 'diplomatie', 'négociation', 'collaboration', 'compromis', 'médiation', 'tolérance', 'intimité', 'amitié', 'amour', 'conflit', 'stress', 'émotion', 'bienveillance', 'éthique relationnelle']\n",
        "\n",
        "# Lexique Histoire\n",
        "lexique_histoire = ['histoire', 'événement', 'période', 'révolution', 'guerre', 'paix', 'civilisation', 'archéologie', 'époque', 'chronologie', 'historien', 'patrimoine', 'document', 'archive', 'mémoire collective', 'colonisation', 'indépendance', 'héritage', 'histoire mondiale']\n",
        "\n",
        "# Lexique Environnement\n",
        "lexique_environnement = ['environnement', 'écologie', 'biodiversité', 'écosystème', 'climat', 'faune', 'flore', 'forêt', 'océan', 'montagne', 'développement durable', 'conservation', 'énergie renouvelable', 'pollution', 'réchauffement climatique', 'sustainability', 'recyclage', 'agriculture durable', 'éco-responsabilité']\n",
        "\n",
        "# Lexique Politique (déjà défini)\n",
        "\n",
        "# Lexique Éducation\n",
        "lexique_education = ['éducation', 'école', 'enseignement', 'professeur', 'élève', 'apprentissage', 'connaissance', 'cours', 'matière', 'éducation supérieure', 'diplôme', 'examen', 'réussite', 'échec', 'pédagogie', 'salle de classe', 'éducatif', 'compétence', 'formation']\n",
        "\n",
        "# Lexique Santé\n",
        "lexique_sante = ['santé', 'bien-être', 'maladie', 'médecine', 'système de santé', 'médecin', 'infirmière', 'prévention', 'traitement', 'nutrition', 'hygiène', 'activité physique', 'psychologie', 'santé mentale', 'maladie chronique', 'pharmacie', 'vaccination', 'santé publique', 'accès aux soins']\n",
        "\n",
        "# Lexique Art\n",
        "lexique_art = ['art', 'œuvre', 'artiste', 'créativité', 'expression', 'beauté', 'culture artistique', 'peinture', 'sculpture', 'photographie', 'architecture', 'musée', 'cinéma', 'théâtre', 'danse', 'musique', 'design', 'arts visuels', 'performances']\n",
        "\n",
        "\n",
        "# Lexique Innovation\n",
        "lexique_innovation = ['innovation', 'technologie', 'créativité', 'invention', 'recherche', 'développement', 'progrès', 'start-up', 'entrepreneuriat', 'idée novatrice', 'disruption', 'avancée', 'nouveau', 'efficacité', 'solutions innovantes', 'changements', 'inventeur', 'scientifique', 'révolution']\n",
        "\n",
        "# Lexique Éthique\n",
        "lexique_ethique = ['éthique', 'morale', 'valeurs', 'intégrité', 'responsabilité', 'respect', 'droits', 'justice', 'équité', 'principes', 'décision éthique', 'conscience', 'comportement éthique', 'bienséance', 'normes morales', 'devoir', 'honnêteté', 'dignité', 'transparence']\n",
        "\n",
        "# Liste des thèmes et leurs mots respectifs\n",
        "themes = {\n",
        "    'Musique': lexique_musique,\n",
        "    'Sport': lexique_sport,\n",
        "    'Politique': lexique_politique,\n",
        "    'Gastronomie': lexique_gastronomie,\n",
        "    'Voyage': lexique_voyage,\n",
        "    'Nature': lexique_nature,\n",
        "    'Technologie': lexique_technologie,\n",
        "    'Culture': lexique_culture,\n",
        "    'Economie': lexique_economie,\n",
        "    'Science': lexique_science,\n",
        "    'Relations': lexique_relations,\n",
        "    'Histoire': lexique_histoire,\n",
        "    'Environnement': lexique_environnement,\n",
        "    'Education': lexique_education,\n",
        "    'Sante': lexique_sante,\n",
        "    'Art': lexique_art,\n",
        "    'Innovation': lexique_innovation,\n",
        "    'Ethique': lexique_ethique\n",
        "}\n",
        "\n",
        "# Liste des keywords à rechercher\n",
        "keywords = ['Musique', 'Sport', 'Politique', 'Gastronomie', 'Voyage', 'Nature', 'Technologie', 'Culture', 'Economie', 'Science', 'Relations', 'Histoire', 'Environnement', 'Politique', 'Education', 'Sante', 'Art', 'Innovation', 'Ethique']\n",
        "\n",
        "def countTheme(Row):\n",
        "  listeTheme=[]\n",
        "  for theme in keywords:\n",
        "    print(theme)\n",
        "    Occurance = 0\n",
        "    for word in Row:\n",
        "      if word in themes[theme]:\n",
        "        Occurance += 1\n",
        "    listeTheme.append(Occurance)\n",
        "  return listeTheme\n",
        "\n",
        "count_theme_udf = udf(countTheme, ArrayType(IntegerType()))"
      ],
      "metadata": {
        "id": "mqtDKBoE9e0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import du WARC"
      ],
      "metadata": {
        "id": "F-OV0bwT9kp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMBTOp_VG90h"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "DIR=\"LIFRANUM\"\n",
        "!mkdir -p $DIR\n",
        "\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/autre $DIR\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/cartoweb $DIR\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/lifranum-method $DIR\n",
        "!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/repo-ecritures-num $DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbtD0JzWIkL1"
      },
      "outputs": [],
      "source": [
        "from aut import *\n",
        "\n",
        "WARCs_path = \"LIFRANUM/repo-ecritures-num/*.warc*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9fC7cw3EjCi"
      },
      "source": [
        "## Récupération des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqn8Em8VEn2Y"
      },
      "source": [
        "### Métadonnées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF-P7XxiV6Lf"
      },
      "outputs": [],
      "source": [
        "df_all = WebArchive(sc, sqlContext, WARCs_path).all()\n",
        "df_all = df_all.filter(df_all['http_status_code'] == 200)\n",
        "df_all = df_all.withColumnRenamed('content', 'content2').withColumnRenamed('url', 'url2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entV9iTkEuf6"
      },
      "source": [
        "### Contenu de la page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMgXPPzooXGu",
        "outputId": "9e257f9c-31d9-440d-f026-e8320b273c59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[crawl_date: string, url: string, mime_type_web_server: string, mime_type_tika: string, language: string, content: string]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_webpages = WebArchive(sc, sqlContext, WARCs_path).webpages()\n",
        "df_webpages = df_webpages.filter(df_webpages['language'] == 'fr')\n",
        "df_webpages.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcsHN421E7rO"
      },
      "source": [
        "### Jointure sur l'url des deux tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPXWjdxlpvKs"
      },
      "outputs": [],
      "source": [
        "df_joined = df_all.join(df_webpages, df_all['url2'] == df_webpages['url'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9puN9Aq6p2s"
      },
      "source": [
        "### Limiting the number of rows to accelerate tests\n",
        "\n",
        "Set **NUMBER_OF_ROWS** at the start to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmVPsuqcySMm"
      },
      "outputs": [],
      "source": [
        "df_joined = df_joined.orderBy(rand()).limit(NUMBER_OF_ROWS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UETZq5cGxgDE"
      },
      "source": [
        "### Adding an ID to facilitate recognising different entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtQsRcEWxgiy"
      },
      "outputs": [],
      "source": [
        "df_joined = df_joined.withColumn(\"id\", monotonically_increasing_id())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-yPW3m2-vwv"
      },
      "outputs": [],
      "source": [
        "df_text = df_joined.withColumn(\"text\", remove_html( remove_http_header(\"content2\") )).withColumn(\"domainName\",get_url_domain_name(df_joined[\"url\"])).select(\"id\",\"domainName\",\"text\").withColumnRenamed(\"domainName\",\"url\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpRbPgzrAqy8",
        "outputId": "202031f7-d89f-4680-ae5c-c538db222109"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, url: string, text: string]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df_text.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSlE6FQFLNr"
      },
      "source": [
        "### Filtrage des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCtudRPyFPSa"
      },
      "source": [
        "### Ajout des stopword français"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk7KUHn8RfSm"
      },
      "outputs": [],
      "source": [
        "stopwordList = nltk.corpus.stopwords.words('french')\n",
        "for char in \"abcdefghijklmnopqrstuvwxyzAZERTYUIOPQSDFGHJKLMWXCVBN1234567890\":\n",
        "  stopwordList.append(char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKXwgv_2P_ps"
      },
      "source": [
        "\n",
        "### Application d'un tokenizer + remover via un pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTwFKgbX5zou"
      },
      "outputs": [],
      "source": [
        "regTokernizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\",pattern=\"\\\\W+\",gaps=True)\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\",stopWords=stopwordList)\n",
        "\n",
        "#word2Vec = Word2Vec(vectorSize=10, inputCol=\"filtered_words\", outputCol=\"vector\")\n",
        "#cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
        "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Q1Pao9P4WC"
      },
      "source": [
        "### Création du pipeline et application sur le dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WsgbdzzNkm9"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=[regTokernizer,remover])\n",
        "# word2Vec\n",
        "model = pipeline.fit(df_text)\n",
        "\n",
        "df_transformed = model.transform(df_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l56K4wgALGM",
        "outputId": "b2818dbe-75d0-40c9-a045-67e8073d8bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+--------------------+--------------------+\n",
            "| id|                 url|                text|               words|      filtered_words|\n",
            "+---+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0|       retailles.com|3f8 décembre | 20...|[3f8, d, cembre, ...|[3f8, cembre, 201...|\n",
            "|  1|www.lionel-seppol...|3a  17 Vigie, fév...|[3a, 17, vigie, f...|[3a, 17, vigie, v...|\n",
            "|  2|yvonpare.blogspot...|353e  Littérature...|[353e, litt, ratu...|[353e, litt, ratu...|\n",
            "|  3|yvonpare.blogspot...|35be  Littérature...|[35be, litt, ratu...|[35be, litt, ratu...|\n",
            "|  4|www.lionel-seppol...|16  Vigie, octobr...|[16, vigie, octob...|[16, vigie, octob...|\n",
            "|  5|lebathyscaphe.blo...|424c Le Bathyscap...|[424c, le, bathys...|[424c, bathyscaph...|\n",
            "|  6|yvonpare.blogspot...|3534  Littérature...|[3534, litt, ratu...|[3534, litt, ratu...|\n",
            "|  7|yvonpare.blogspot...|352e  Littérature...|[352e, litt, ratu...|[352e, litt, ratu...|\n",
            "|  8|lesmainsvides.blo...|2e37 Les mains vi...|[2e37, les, mains...|[2e37, mains, vid...|\n",
            "|  9|www.lionel-seppol...|177  Belledonne |...|[177, belledonne,...|[177, belledonne,...|\n",
            "+---+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDoj9uybGK9z"
      },
      "source": [
        "### Transformations additionelles du DF\n",
        "\n",
        "\n",
        "\n",
        "*   Verification que les mots sont bien dans le dictionnaire\n",
        "*   Analyse des Potentielles catégories de chaque terme\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.drop(df_transformed.text).drop(df_transformed.words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-qvBJDzFhXx",
        "outputId": "fd49085e-9f03-4374-8d68-c0b41620ce3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, url: string, filtered_words: array<string>]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Nettoyage avec dictionnaire\n",
        "2. Comptage des mots\n",
        "3. Assignation à des categories\n",
        "4. Ajout de sentiments"
      ],
      "metadata": {
        "id": "4QxNsKKOF46N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8pLoAsYVKMF",
        "outputId": "f97d6895-8858-4a18-f2bf-70a09ea3fcc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, url: string, text: string, words: array<string>, filtered_words: array<string>, clean_text_in_dict: string, count_clean_words: int, categories: array<int>, sentiment: string]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "df_transformed = df_transformed.withColumn(\"clean_text_in_dict\",word_in_dict_udf(concat_ws(\" \",df_transformed[\"filtered_words\"])))\n",
        "#df_transformed = df_transformed.withColumn(\"categories\", categories_text(df_transformed[\"clean_text_in_dict\"]))\n",
        "#df_transformed = df_transformed.withColumn(\"most_frequent_word\", most_frequent_udf(df_transformed[\"categories\"]))\n",
        "df_transformed = df_transformed.withColumn(\"count_clean_words\", count_words_udf(df_transformed[\"clean_text_in_dict\"]))\n",
        "df_transformed = df_transformed.withColumn(\"categories\", count_theme_udf(df_transformed[\"filtered_words\"]))\n",
        "df_transformed = df_transformed.withColumn(\"sentiment\", sentiment_udf(df_transformed[\"clean_text_in_dict\"]))\n",
        "\n",
        "df_transformed.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zu_9OOLmBC6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.show(10, False)"
      ],
      "metadata": {
        "id": "0feb3cqRBDRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mKQbcJIGrKj"
      },
      "source": [
        "## Export du code vers Pandas pour affichage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyv8CcNGvwk5"
      },
      "outputs": [],
      "source": [
        "df_export = df_transformed.select(\"id\",\"url\",\"count_clean_words\",\"categories\",\"sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf9KotSAvvFv"
      },
      "outputs": [],
      "source": [
        "df_export.cache()\n",
        "#df_export.show(10,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUp6PzMqfIgV"
      },
      "outputs": [],
      "source": [
        "pandasSentiment = df_export.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgImjrrHk8bo"
      },
      "outputs": [],
      "source": [
        "pandasSentiment.to_csv(\"result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7Vtywg8nQR4"
      },
      "outputs": [],
      "source": [
        "pandasSentiment['sentiment'] = pandasSentiment['sentiment'].apply(pd.to_numeric, errors='coerce')\n",
        "pandasSentiment[\"roundSentiment\"] = round(pandasSentiment[\"sentiment\"], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_8GOo-WpzM1"
      },
      "outputs": [],
      "source": [
        "pandasSentiment.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNB6IlC1c60c"
      },
      "outputs": [],
      "source": [
        "pandasSentiment.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy05jeD6pUt2"
      },
      "outputs": [],
      "source": [
        "pandasSentimentTContent = pandasSentiment[pandasSentiment[\"roundSentiment\"] > 0.15]\n",
        "pd.set_option('max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV7-Mu9UN6Fr"
      },
      "outputs": [],
      "source": [
        "#  pandasSentimentTContent.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuuplOXTWXNw"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(pandasSentiment.sort_values(by=\"roundSentiment\",ascending=True),x=\"roundSentiment\")\n",
        "\n",
        "fig.update_traces(xbins=dict( # bins used for histogram\n",
        "        start=-0.4,\n",
        "        end=0.5,\n",
        "        size=0.03\n",
        "    ))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-rGbYH6Snln"
      },
      "source": [
        "## Similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cré"
      ],
      "metadata": {
        "id": "e4tWtqUJCrTC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmQxRMnZbN9x"
      },
      "outputs": [],
      "source": [
        "dot = graphviz.Digraph('similarities', comment='Subject and emotions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkvhZpLSo1aK"
      },
      "outputs": [],
      "source": [
        "nodes = df_sentiment.select(\"id\",\"url\",\"most_frequent_word\",\"sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-KGtoV-rja1"
      },
      "outputs": [],
      "source": [
        "nodes.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfUx2dM8o2nx"
      },
      "outputs": [],
      "source": [
        "nodes = nodes.withColumn(\"color\",convert_to_color(nodes[\"sentiment\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H47rJK7uDMi"
      },
      "outputs": [],
      "source": [
        "nodes.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWpqMNi2aeUD"
      },
      "outputs": [],
      "source": [
        "for row in nodes.collect():\n",
        "  dot.node(str(row.id),row.url+\" : \"+row.most_frequent_word,color=row.color,style='filled',fillcolor=row.color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfRFODhkSxDY"
      },
      "outputs": [],
      "source": [
        "df = df_sentiment.select(\"id\",\"vector\")\n",
        "df2 = df.withColumnRenamed(\"id\",\"id2\").withColumnRenamed(\"vector\",\"vector2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KIm9rViJ3kg"
      },
      "outputs": [],
      "source": [
        "# Assume that df is a DataFrame with a column \"features\" that contains the vectors\n",
        "df = df.join(df2, df[\"id\"] != df2[\"id2\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riCRyOieJ430"
      },
      "outputs": [],
      "source": [
        "df.show(10,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXLVAHtPSmn-"
      },
      "outputs": [],
      "source": [
        "# Calculate the cosine similarity between each pair of vectors\n",
        "df = df.withColumn(\"similarity\", cosine_similarity_udf(\"vector\", \"vector2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlBG1UCqJi2R"
      },
      "outputs": [],
      "source": [
        "#df.show(10,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw_BiJADKGZn"
      },
      "outputs": [],
      "source": [
        "# Find the most similar vector for each vector\n",
        "#df = df.groupBy(\"i.id\").agg({\"similarity\": \"max\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl3h9V5KFNCC"
      },
      "outputs": [],
      "source": [
        "# Create the edge DataFrame\n",
        "edges = df.filter(df[\"similarity\"] > MIN_SIMILARITY_COEF).select(\"id\",\"id2\",\"similarity\").withColumnRenamed(\"id\",\"src\").withColumnRenamed(\"id2\",\"dst\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLxU8FT9S6fH"
      },
      "outputs": [],
      "source": [
        "edges.show(10,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTLicPYvcRn-"
      },
      "outputs": [],
      "source": [
        "for row in edges.collect():\n",
        "  if(row.dst > row.src):\n",
        "    dot.edge(str(row.src),str(row.dst),dir=\"both\")\n",
        "  # ,label=str(row.similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt8Nm64ddOSG"
      },
      "outputs": [],
      "source": [
        "dot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dot.render(filename='g1')"
      ],
      "metadata": {
        "id": "BmgbPHzPWDaP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}